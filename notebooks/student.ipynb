{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba3dd4c",
   "metadata": {},
   "source": [
    "# 2026 QMW - Genomics Module\n",
    "\n",
    "January 5, 2026\n",
    "\n",
    "Author: Erik Owen | Graduate Student in Computational & Systems Biology | Page Lab\n",
    "\n",
    "# Goals:\n",
    "\n",
    "- QC summary from FASTQ\n",
    "- Alignment (BAM) & index\n",
    "- Coverage & MAPQ summaries\n",
    "- Variant calls (VCF)\n",
    "- Comparison to ground truth VCF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a6d3e",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "- download software tools + packages that we'll use in this tutorial\n",
    "- download toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba6024",
   "metadata": {},
   "source": [
    "### What does `!` mean?\n",
    "\n",
    "In Colab/Jupyter, you'll see lines that start with an exclamation mark, like `!ls` or `!bowtie2 --version`.\n",
    "\n",
    "`!` tells the notebook: “run the rest of this line as a shell command (like you typed it in a Terminal), not as Python.”\n",
    "\n",
    "- `!ls` lists files in the current folder\n",
    "- `!pwd` prints the current folder path\n",
    "- `!curl ...` is a command-line tool for data transfer\n",
    "- `!zcat file.fastq.gz | head` runs a small pipeline of command-line tools\n",
    "  - `|` is called a \"pipe\". This sends the output of the command on the left directly into the command on the right so you can \"stream\" results without creating intermediate files.\n",
    "\n",
    "*Important*: `!` only works inside notebook cells. If you copy the same line into a .sh bash script, you must remove the `!` (because scripts are already “shell mode”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee05283",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L https://raw.githubusercontent.com/erik-owen/2026_QMW_Genomics/main/scripts/bootstrap_colab.sh | bash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17d506",
   "metadata": {},
   "source": [
    "The `tree` command is a UNIX command-line utility that shows your directory + file structure.\n",
    "\n",
    "We'll be using it throughout the notebook to show you what files we're creating + using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8de53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585bcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select case study inputs\n",
    "CASE = 1  #@param {type:\"integer\", min:1, max:2}\n",
    "SAMPLE = \"ind1_noHbS\"  #@param [\"ind1_noHbS\", \"ind2_HbS_carrier\", \"ind1_repeat_mix\"]\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data\") / \"loci\"\n",
    "assert DATA.exists(), f\"Missing {DATA}. Did the bootstrap step run?\"\n",
    "\n",
    "FILES = {\n",
    "    (1, \"ind1_noHbS\"): dict(\n",
    "        ref=DATA / \"locus1_hg38_chr11_5215464_5237071_HBB.fa\",\n",
    "        reads=DATA / \"locus1_HBB.ind1_noHbS.reads_R1.fastq.gz\",\n",
    "        repeats=DATA / \"locus1_hg38_chr11_5215464_5237071_HBB.repeats.local.bed\",\n",
    "    ),\n",
    "    (1, \"ind2_HbS_carrier\"): dict(\n",
    "        ref=DATA / \"locus1_hg38_chr11_5215464_5237071_HBB.fa\",\n",
    "        reads=DATA / \"locus1_HBB.ind2_HbS_carrier.reads_R1.fastq.gz\",\n",
    "        repeats=DATA / \"locus1_hg38_chr11_5215464_5237071_HBB.repeats.local.bed\",\n",
    "    ),\n",
    "    (2, \"ind1_repeat_mix\"): dict(\n",
    "        ref=DATA / \"locus2_hg38_chr8_96243643_96263643_MTERF3.fa\",\n",
    "        reads=DATA / \"locus2_MTERF3_repeat.ind1_repeat_mix.reads_R1.fastq.gz\",\n",
    "        repeats=DATA / \"locus2_hg38_chr8_96243643_96263643_MTERF3.repeats.local.bed\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "paths = FILES[(CASE, SAMPLE)]\n",
    "for k, p in paths.items():\n",
    "    assert p.exists(), f\"Missing {k}: {p}\"\n",
    "paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1852fc2",
   "metadata": {},
   "source": [
    "# Sequencing & File formats\n",
    "\n",
    "## Inpsect a FASTA file\n",
    "\n",
    "FASTA files is a text-based format for representing a sequence.\n",
    "\n",
    "It has two parts:\n",
    "- header: begins with `>`, + single line description of sequence\n",
    "- sequence representation: 1 letter per nucleic acid\n",
    "\n",
    "You've been provided 2 FASTA files. Go ahead and inpsect them using the command line:\n",
    "\n",
    "- locus1_hg38_chr11_5215464_5237071_HBB.fa\n",
    "- locus2_hg38_chr8_96243643_96263643_MTERF3.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e22405",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"FASTA heads:\"\n",
    "!echo\n",
    "!head -n 3 data/loci/locus1_hg38_chr11_5215464_5237071_HBB.fa\n",
    "!echo\n",
    "!head -n 3 data/loci/locus2_hg38_chr8_96243643_96263643_MTERF3.fa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8989b8f",
   "metadata": {},
   "source": [
    "Now, using `python`, inspect our first case study. Return:\n",
    "- FASTA header\n",
    "- sequence length\n",
    "- GC% content\n",
    "- first 80 base pairs\n",
    "\n",
    "Note: we've stored the paths to our data in: the paths dict. You want to select the `*.fa` file under `paths[\"ref\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bde25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # package for regular expressions\n",
    "\n",
    "def read_fasta_one(path):\n",
    "    header = None\n",
    "    seq_chunks = []\n",
    "    with open(path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                header = line[1:]\n",
    "            else:\n",
    "                seq_chunks.append(line)\n",
    "    if header is None:\n",
    "        raise ValueError(f\"No FASTA header found in {path}\")\n",
    "    seq = \"\".join(seq_chunks).upper()\n",
    "    return header, seq\n",
    "\n",
    "def gc_fraction(seq: str) -> float:\n",
    "    gc = sum(1 for b in seq if b in (\"G\",\"C\"))\n",
    "    atgc = sum(1 for b in seq if b in (\"A\",\"T\",\"G\",\"C\"))\n",
    "    return (gc / atgc) if atgc else float(\"nan\")\n",
    "\n",
    "ref_path = paths[\"ref\"]\n",
    "header, ref_seq = read_fasta_one(ref_path)\n",
    "\n",
    "print(\"FASTA:\", ref_path.name)\n",
    "print(\"Header:\", header)\n",
    "print(\"Length (bp):\", len(ref_seq))\n",
    "print(\"GC fraction:\", round(gc_fraction(ref_seq), 4))\n",
    "print(\"First 80 bp:\", ref_seq[:80])\n",
    "\n",
    "# Extract chr/start/end if present in header or filename\n",
    "m = re.search(r\"(chr[\\w]+)[:_](\\d+)[-_](\\d+)\", header) or re.search(r\"(chr[\\w]+)_(\\d+)_(\\d+)\", ref_path.name)\n",
    "if m:\n",
    "    chrom, start, end = m.group(1), int(m.group(2)), int(m.group(3))\n",
    "    print(\"Parsed locus:\", chrom, start, end, \"(span:\", end-start, \"bp )\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932126c",
   "metadata": {},
   "source": [
    "Does this FASTA represent:\n",
    "- the whole human genome?\n",
    "- 1 chromosome?\n",
    "- a small region?\n",
    "\n",
    "Why would your above answer be appropriate for a class setting?\n",
    "\n",
    "Answer in MITx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab154103",
   "metadata": {},
   "source": [
    "## Inspect a FASTQ file\n",
    "\n",
    "What do reads look like?\n",
    "\n",
    "Using the command line, here are the first 2 records in our FASTQ file.\n",
    "\n",
    "FASTQ files are composed of 4 lines. What do they signify?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat data/loci/locus1_HBB.ind1_noHbS.reads_R1.fastq.gz | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e994ce",
   "metadata": {},
   "source": [
    "Preview a few FASTQ records, then compute:\n",
    "\n",
    "- number of reads\n",
    "- read length\n",
    "- quick quality summary + plot\n",
    "\n",
    "In the next cell, please implement the function `phred33_to_q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7433adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phred33_to_q(qual: str):\n",
    "  \"\"\"\n",
    "  Convert a FASTQ quality string (Phred+33 encoding) into numeric Q scores.\n",
    "\n",
    "  Inputs:\n",
    "  qual is string, see the assert checks for examples\n",
    "  # See https://en.wikipedia.org/wiki/Phred_quality_score#Symbols for more\n",
    "\n",
    "  Each character encodes a base quality:\n",
    "    Q = ord(character) - 33\n",
    "\n",
    "  Example:\n",
    "    \"I!\" -> [40, 0]\n",
    "\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  list[int]\n",
    "      One Q score per base; same length as `qual`.\n",
    "  \"\"\"\n",
    "  # TODO\n",
    "  # hint: use list comprehension for one-liner\n",
    "  return # TODO\n",
    "\n",
    "assert phred33_to_q(\"!\") == [0]\n",
    "assert phred33_to_q(\"I\") == [40]\n",
    "assert phred33_to_q(\"?\") == [30]\n",
    "assert phred33_to_q(\"I!I?\") == [40, 0, 40, 30]\n",
    "print(\"phred33_to_q sanity checks passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae56de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from statistics import mean\n",
    "\n",
    "def fastq_iter_gz(path):\n",
    "  opener = gzip.open if str(path).endswith(\".gz\") else open\n",
    "  with opener(path, \"rt\") as f:\n",
    "    while True:\n",
    "      h = f.readline().rstrip()\n",
    "      if not h:\n",
    "        return\n",
    "      s = f.readline().rstrip()\n",
    "      plus = f.readline().rstrip()\n",
    "      q = f.readline().rstrip()\n",
    "      yield h, s, plus, q\n",
    "\n",
    "reads_path = paths[\"reads\"]\n",
    "\n",
    "# Preview first 3 records\n",
    "it = fastq_iter_gz(reads_path)\n",
    "for i in range(3):\n",
    "    h, s, plus, q = next(it)\n",
    "    print(\"---- record\", i+1, \"----\")\n",
    "    print(h)\n",
    "    print(\"seq (first 60):\", s[:60], \"...\")\n",
    "    print(\"qual(first 60):\", q[:60], \"...\")\n",
    "    print(\"len:\", len(s), \"meanQ:\", round(mean(phred33_to_q(q)), 2))\n",
    "\n",
    "# Stream stats\n",
    "n_reads = 0\n",
    "lengths = []\n",
    "mean_qs = []\n",
    "\n",
    "for h, s, plus, q in fastq_iter_gz(reads_path):\n",
    "    n_reads += 1\n",
    "    lengths.append(len(s))\n",
    "    mean_qs.append(mean(phred33_to_q(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MITx checkpoint: submit the number of reads in this FASTQ.\")\n",
    "print(\"Sanity check: this is on the order of 10^3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc1545",
   "metadata": {},
   "source": [
    "### Per-base quality profile\n",
    "\n",
    "Now, we want to explore the quality of our reads which is given by our Phred quality scores:\n",
    "\n",
    "$Q=-10\\log_{10} P$\n",
    "\n",
    "Where $P$ is the probability of an calling that base incorrectly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4474783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for Q in [10, 20, 30, 40]:\n",
    "    p = 10 ** (-Q/10)\n",
    "    print(f\"Q={Q}: error probability ≈ {p:g} (≈ 1 in {round(1/p):,} bases)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MITx checkpoint: What is the mean Q per-read in our raw FASTQ file?\")\n",
    "print(\"Does this make you worry about the errors? Think about the frequency of SNPs in a given human.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fastq_iter_gz(path):\n",
    "    opener = gzip.open if str(path).endswith(\".gz\") else open\n",
    "    with opener(path, \"rt\") as f:\n",
    "        while True:\n",
    "            h = f.readline().rstrip()\n",
    "            if not h:\n",
    "                return\n",
    "            s = f.readline().rstrip()\n",
    "            plus = f.readline().rstrip()\n",
    "            q = f.readline().rstrip()\n",
    "            yield h, s, plus, q\n",
    "\n",
    "def per_base_q_profile(fastq_path, min_count_per_cycle=50):\n",
    "    \"\"\"\n",
    "    Returns dict with x (1-based cycles), mean, p10, p90, n_reads.\n",
    "    For variable-length reads (trimmed), cycles with <min_count_per_cycle are dropped.\n",
    "    \"\"\"\n",
    "    per_cycle = []  # list of lists of Q values\n",
    "    n_reads = 0\n",
    "\n",
    "    for _, s, _, qstr in fastq_iter_gz(fastq_path):\n",
    "        q = phred33_to_q(qstr)\n",
    "        if len(per_cycle) < len(q):\n",
    "            per_cycle.extend([[] for _ in range(len(q) - len(per_cycle))])\n",
    "        for i, qi in enumerate(q):\n",
    "            per_cycle[i].append(int(qi))\n",
    "        n_reads += 1\n",
    "\n",
    "    # Convert to arrays and compute stats, keeping only cycles with enough observations\n",
    "    means, p10s, p90s, xs = [], [], [], []\n",
    "    for i, vals in enumerate(per_cycle):\n",
    "        if len(vals) < min_count_per_cycle:\n",
    "            continue\n",
    "        v = np.array(vals, dtype=np.int16)\n",
    "        xs.append(i + 1)  # 1-based cycle\n",
    "        means.append(float(v.mean()))\n",
    "        p10s.append(float(np.percentile(v, 10)))\n",
    "        p90s.append(float(np.percentile(v, 90)))\n",
    "\n",
    "    return {\n",
    "        \"x\": np.array(xs, dtype=int),\n",
    "        \"mean\": np.array(means, dtype=float),\n",
    "        \"p10\": np.array(p10s, dtype=float),\n",
    "        \"p90\": np.array(p90s, dtype=float),\n",
    "        \"n_reads\": n_reads,\n",
    "    }\n",
    "\n",
    "# Paths\n",
    "reads_raw = paths[\"reads\"]\n",
    "raw = per_base_q_profile(reads_raw, min_count_per_cycle=50)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Raw: gray band + black mean line\n",
    "plt.fill_between(raw[\"x\"], raw[\"p10\"], raw[\"p90\"], alpha=0.20, label=\"raw (10–90%)\")\n",
    "plt.plot(raw[\"x\"], raw[\"mean\"], linewidth=2, label=\"raw mean\")\n",
    "\n",
    "plt.xlabel(\"Read position (cycle)\")\n",
    "plt.ylabel(\"Phred Q\")\n",
    "plt.title(\"Per-base quality: raw reads from FASTQ file\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Raw reads processed:\", raw[\"n_reads\"])\n",
    "print(\"Raw mean Q by cycle:    min/median/max =\",\n",
    "      round(raw[\"mean\"].min(),2), round(np.median(raw[\"mean\"]),2), round(raw[\"mean\"].max(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb0338",
   "metadata": {},
   "source": [
    "Does the quality drop at the 3' end? Why might you think so?\n",
    "\n",
    "Hint: Think back to sequencing-by-synthesis and the optical readout!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86615a21",
   "metadata": {},
   "source": [
    "## Quality trimming\n",
    "\n",
    "Here, we are going to use the tool TrimGalore to remove low quality portions of our read!\n",
    "\n",
    "This is a pretty typical part of the QC pipeline\n",
    "\n",
    "https://github.com/FelixKrueger/TrimGalore/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca3e9f",
   "metadata": {},
   "source": [
    "### Trim all FASTQs in toy data\n",
    "\n",
    "This precomputes all TrimGalore outputs for all toy FASTQs.\n",
    "\n",
    "This way we don't have to redo this section for the other provided FASTQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# TRY ADJUSTING THESE PARAMETERS AND RERUN!\n",
    "# What happend downstream?\n",
    "Q_TRIM = 20\n",
    "MIN_LEN = 50\n",
    "\n",
    "WORK = Path(\"work\")\n",
    "WORK.mkdir(exist_ok=True)\n",
    "\n",
    "def work_dir(case: int, sample: str) -> Path:\n",
    "    d = WORK / f\"case{case}_{sample}\"\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def run_trim_galore(case: int, sample: str) -> Path:\n",
    "    reads = FILES[(case, sample)][\"reads\"]\n",
    "    outdir = work_dir(case, sample) / \"trim_galore\"\n",
    "\n",
    "    # Clear and rerun\n",
    "    if outdir.exists():\n",
    "        shutil.rmtree(outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n=== Trimming (rebuild): case{case} / {sample} ===\")\n",
    "    print(\"Input:\", reads)\n",
    "    print(\"Outdir:\", outdir)\n",
    "    print(f\"Params: Q_TRIM={Q_TRIM}, MIN_LEN={MIN_LEN}\")\n",
    "\n",
    "    # Run Trim Galore (single-end)\n",
    "    # !!! IMPORTANT !!!\n",
    "    !trim_galore --quality {Q_TRIM} --length {MIN_LEN} --fastqc --gzip -o {outdir} {reads}\n",
    "\n",
    "    trimmed = sorted(outdir.glob(\"*_trimmed.f*q.gz\"))\n",
    "    assert trimmed, f\"Trim Galore finished but no trimmed FASTQ found in {outdir}\"\n",
    "\n",
    "    # If multiple (unexpected for single-end), pick the newest\n",
    "    trimmed_path = max(trimmed, key=lambda p: p.stat().st_mtime)\n",
    "    return trimmed_path\n",
    "\n",
    "# Run for all defined samples\n",
    "trimmed_map = {}\n",
    "for (case, sample) in FILES.keys():\n",
    "    trimmed_path = run_trim_galore(case, sample)\n",
    "    trimmed_map[(case, sample)] = trimmed_path\n",
    "    print(\"Trimmed FASTQ:\", trimmed_path)\n",
    "\n",
    "print(\"\\nDone. Trimmed files available for later alignment:\")\n",
    "trimmed_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9dadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cceefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = work_dir(CASE, SAMPLE) / \"trim_galore\"\n",
    "\n",
    "trimmed = sorted(outdir.glob(\"*_trimmed.fq.gz\")) + sorted(outdir.glob(\"*_trimmed.fastq.gz\"))\n",
    "assert len(trimmed) == 1, f\"Expected 1 trimmed file, found: {trimmed}\"\n",
    "reads_trimmed = trimmed[0]\n",
    "reads_trimmed\n",
    "\n",
    "# Base data folder\n",
    "DATA = Path(\"data\") / \"loci\"\n",
    "assert DATA.exists(), f\"Missing {DATA}. Did the bootstrap step run?\"\n",
    "\n",
    "# Map (CASE, SAMPLE) -> input reads path\n",
    "READS = {\n",
    "    (1, \"ind1_noHbS\"): DATA / \"locus1_HBB.ind1_noHbS.reads_R1.fastq.gz\",\n",
    "    (1, \"ind2_HbS_carrier\"): DATA / \"locus1_HBB.ind2_HbS_carrier.reads_R1.fastq.gz\",\n",
    "    (2, \"ind1_repeat_mix\"): DATA / \"locus2_MTERF3_repeat.ind1_repeat_mix.reads_R1.fastq.gz\",\n",
    "}\n",
    "# Input reads for the current selection\n",
    "reads_in = READS[(CASE, SAMPLE)]\n",
    "assert reads_in.exists(), f\"Missing reads_in: {reads_in}\"\n",
    "\n",
    "import gzip, numpy as np\n",
    "\n",
    "def fastq_stats_gz(path):\n",
    "    n = 0\n",
    "    lens = []\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        while True:\n",
    "            h = f.readline()\n",
    "            if not h:\n",
    "                break\n",
    "            s = f.readline().rstrip()\n",
    "            f.readline()\n",
    "            f.readline()\n",
    "            n += 1\n",
    "            lens.append(len(s))\n",
    "    lens = np.array(lens)\n",
    "    return n, int(lens.min()), float(lens.mean()), int(lens.max())\n",
    "\n",
    "n0, mn0, av0, mx0 = fastq_stats_gz(reads_in)\n",
    "n1, mn1, av1, mx1 = fastq_stats_gz(reads_trimmed)\n",
    "\n",
    "print(\"RAW    reads:\", n0, \"len min/mean/max:\", mn0, round(av0,2), mx0)\n",
    "print(\"TRIM   reads:\", n1, \"len min/mean/max:\", mn1, round(av1,2), mx1)\n",
    "\n",
    "# Checkpoint\n",
    "print(\"MITx checkpoint: submit trimmed read count and mean trimmed length.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca96ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "reads_raw = paths[\"reads\"]\n",
    "reads_trimmed = reads_trimmed  # from your Trim Galore step\n",
    "\n",
    "raw = per_base_q_profile(reads_raw, min_count_per_cycle=50)\n",
    "trim = per_base_q_profile(reads_trimmed, min_count_per_cycle=50)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Raw: gray band + black mean line\n",
    "plt.fill_between(raw[\"x\"], raw[\"p10\"], raw[\"p90\"], alpha=0.20, label=\"raw (10–90%)\")\n",
    "plt.plot(raw[\"x\"], raw[\"mean\"], linewidth=2, label=\"raw mean\")\n",
    "\n",
    "# Trimmed: blue band + blue mean line\n",
    "# (Matplotlib default blue; no need to specify if you prefer default)\n",
    "plt.fill_between(trim[\"x\"], trim[\"p10\"], trim[\"p90\"], alpha=0.20, label=\"trimmed (10–90%)\")\n",
    "plt.plot(trim[\"x\"], trim[\"mean\"], linewidth=2, label=\"trimmed mean\")\n",
    "\n",
    "plt.xlabel(\"Read position (cycle)\")\n",
    "plt.ylabel(\"Phred Q\")\n",
    "plt.title(\"Per-base quality: raw vs trimmed\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Raw reads processed:\", raw[\"n_reads\"])\n",
    "print(\"Trimmed reads processed:\", trim[\"n_reads\"])\n",
    "print(\"Raw mean Q by cycle:    min/median/max =\",\n",
    "      round(raw[\"mean\"].min(),2), round(np.median(raw[\"mean\"]),2), round(raw[\"mean\"].max(),2))\n",
    "print(\"Trimmed mean Q by cycle: min/median/max =\",\n",
    "      round(trim[\"mean\"].min(),2), round(np.median(trim[\"mean\"]),2), round(trim[\"mean\"].max(),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872eb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lengths(fastq_path):\n",
    "    lens = []\n",
    "    for _, s, _, _ in fastq_iter_gz(fastq_path):\n",
    "        lens.append(len(s))\n",
    "    return np.array(lens, dtype=int)\n",
    "\n",
    "lens_raw  = read_lengths(reads_raw)\n",
    "lens_trim = read_lengths(reads_trimmed)\n",
    "\n",
    "med_raw  = int(np.median(lens_raw))\n",
    "med_trim = int(np.median(lens_trim))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Plot histograms and capture the returned patches to reuse their colors\n",
    "counts_raw, bins_raw, patches_raw = plt.hist(lens_raw, bins=20, alpha=0.35, label=\"raw\")\n",
    "counts_tr,  bins_tr,  patches_tr  = plt.hist(lens_trim, bins=20, alpha=0.35, label=\"trimmed\")\n",
    "\n",
    "raw_color  = patches_raw[0].get_facecolor()\n",
    "trim_color = patches_tr[0].get_facecolor()\n",
    "\n",
    "# Median lines in matching colors\n",
    "plt.axvline(med_raw,  linewidth=2, color=raw_color,  label=f\"raw median = {med_raw} bp\")\n",
    "plt.axvline(med_trim, linewidth=2, color=trim_color, label=f\"trimmed median = {med_trim} bp\")\n",
    "\n",
    "plt.xlabel(\"Read length (bp)\")\n",
    "plt.ylabel(\"Number of reads\")\n",
    "plt.title(\"Read length distribution: raw vs trimmed\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Raw lengths:     n/min/median/mean/max =\",\n",
    "      len(lens_raw), lens_raw.min(), med_raw, round(lens_raw.mean(),2), lens_raw.max())\n",
    "print(\"Trimmed lengths: n/min/median/mean/max =\",\n",
    "      len(lens_trim), lens_trim.min(), med_trim, round(lens_trim.mean(),2), lens_trim.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197d39f",
   "metadata": {},
   "source": [
    "### Run the following cell to get the full QC report from running `fastQC` as part of `TrimGalore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a856bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "\n",
    "# Feel free to change to other CASE and SAMPLE to inspect other files!\n",
    "outdir = work_dir(CASE, SAMPLE) / \"trim_galore\"\n",
    "trim_dir = outdir\n",
    "\n",
    "# Find the HTML report\n",
    "html_candidates = sorted(trim_dir.glob(\"*_fastqc.html\"))\n",
    "assert html_candidates, f\"No *_fastqc.html found in {trim_dir}. Did you run Trim Galore with --fastqc?\"\n",
    "\n",
    "html_path = html_candidates[0]\n",
    "\n",
    "print(\"FastQC report found at:\", html_path)\n",
    "print(\"Instructions:\")\n",
    "print(\"1) This notebook will download the FastQC HTML report to your computer.\")\n",
    "print(\"2) Open the downloaded file (fastqc_report.html) in your web browser.\")\n",
    "print(\"3) Skim these sections: Per base sequence quality, Per sequence quality scores,\")\n",
    "print(\"   Per base sequence content, Sequence Length Distribution\")\n",
    "\n",
    "# Trigger download\n",
    "files.download(str(html_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a2026",
   "metadata": {},
   "source": [
    "## Finish answering questions in the MITx section before moving on to alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41a227",
   "metadata": {},
   "source": [
    "# Alignment\n",
    "\n",
    "\n",
    "We will use `bowtie2` to align our trimmed reads to our reference FASTA.\n",
    "\n",
    "In this module you will:\n",
    "1. Build a Bowtie2 index from a small reference FASTA (a single locus).\n",
    "2 . Align trimmed reads to that reference using Bowtie2.\n",
    "3. Convert the alignments to BAM (binary), sort them, and index them (so tools can query them efficiently).\n",
    "4. Inspect real alignments and learn to interpret:\n",
    "    - CIGAR (match/indel/soft clip)\n",
    "    - MAPQ (confidence in placement)\n",
    "5. Compute quick summaries:\n",
    "  - mapping rate\n",
    "  - MAPQ histogram\n",
    "  - coverage across the locus\n",
    "6. Run a mini-experiment: change Bowtie2 settings and see what changes.\n",
    "\n",
    "\n",
    "Key ideas to keep in mind\n",
    "- Alignment is an optimization problem: Bowtie2 tries to find the best placement(s) for each read.\n",
    "- Repeats are tricky because the “best placement” may not be unique.\n",
    "- MAPQ is about confidence in placement, not base-call quality.\n",
    "\n",
    "Let's start with the CASE and SAMPLE we defined at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "WORK = Path(\"work\")\n",
    "WORK.mkdir(exist_ok=True)\n",
    "\n",
    "# Where trim_galore outputs went\n",
    "case_dir = WORK / f\"case{CASE}_{SAMPLE}\"\n",
    "trim_dir = case_dir / \"trim_galore\"\n",
    "assert trim_dir.exists(), f\"Missing {trim_dir}. Run trimming first.\"\n",
    "\n",
    "ref_fa = FILES[(CASE, SAMPLE)][\"ref\"]\n",
    "\n",
    "# Find the trimmed FASTQ produced by trim_galore (single-end)\n",
    "trimmed = list(trim_dir.glob(\"*_trimmed.fq.gz\"))\n",
    "assert len(trimmed) == 1, f\"Expected 1 trimmed FASTQ in {trim_dir}, found: {trimmed}\"\n",
    "trimmed_fq = trimmed[0]\n",
    "\n",
    "print(\"CASE:\", CASE, \"SAMPLE:\", SAMPLE)\n",
    "print(\"REF:\", ref_fa)\n",
    "print(\"TRIMMED:\", trimmed_fq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d43b01",
   "metadata": {},
   "source": [
    "## Build bowtie2 index\n",
    "\n",
    "We build a **search index** for the locus. Otherwise, we wouldn't have anything to align to!\n",
    "\n",
    "The following command `bowtie2-build` creates the index prefix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0179636",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dir = case_dir / \"bowtie2_index\"\n",
    "idx_dir.mkdir(exist_ok=True)\n",
    "idx_prefix = idx_dir / \"ref\"\n",
    "\n",
    "!bowtie2-build \"{ref_fa}\" \"{idx_prefix}\"\n",
    "!ls -lh \"{idx_dir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008cbfb2",
   "metadata": {},
   "source": [
    "## Align reads\n",
    "\n",
    "We will run 2 alignments to compare:\n",
    "\n",
    "- `Bowtie2` default which does end-to-end pairing\n",
    "- `--very-sensitive-local`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f69955",
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_dir = case_dir / \"alignments\"\n",
    "aln_dir.mkdir(exist_ok=True)\n",
    "\n",
    "sam_default = aln_dir / \"aln.default.sam\"\n",
    "sam_vsl     = aln_dir / \"aln.vslocal.sam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Default alignment (end-to-end)\n",
    "!bowtie2 -x \"{idx_prefix}\" -U \"{trimmed_fq}\" -S \"{sam_default}\" 2> \"{sam_default}.log\"\n",
    "\n",
    "# 2) Very-sensitive LOCAL alignment\n",
    "!bowtie2 --very-sensitive-local -x \"{idx_prefix}\" -U \"{trimmed_fq}\" -S \"{sam_vsl}\" 2> \"{sam_vsl}.log\"\n",
    "\n",
    "# Show Bowtie2 summaries (mapping rate, etc.)\n",
    "!tail -n 20 \"{sam_default}.log\"\n",
    "!echo \"----\"\n",
    "!tail -n 20 \"{sam_vsl}.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2330d5",
   "metadata": {},
   "source": [
    "Try changing Bowtie2 settings to rerun previous step:\n",
    "\n",
    "\n",
    "Ideas to try:\n",
    "- `--local vs default` (end-to-end-ish)\n",
    "- `--very-sensitive` (end-to-end sensitivity preset)\n",
    "- `--very-sensitive-local` (local + sensitive)\n",
    "\n",
    "Change the number of reporting alignments:\n",
    "- `-k 1` (report 1 alignment)\n",
    "- `-k 5` (report up to 5 alignments per read)\n",
    "\n",
    "\n",
    "Read the docs here for guidance! https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#command-line\n",
    "\n",
    "Does the mapping rate change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d372f63",
   "metadata": {},
   "source": [
    "## Convert SAM to BAM and provide mapping summaries\n",
    "\n",
    "- SAM $:=$ sequence alignment map\n",
    "- BAM $:=$ binary alignment map\n",
    "\n",
    "SAM is a text-based format for storing sequences aligned to a reference sequence.\n",
    "\n",
    "BAM is the lossless, compressed binary version of a SAM\n",
    "\n",
    "\n",
    "Why do we index?\n",
    "\n",
    "- Quickly retrive alignments that overlap specific location\n",
    "- We sort our files to do this\n",
    "\n",
    "\n",
    "Overview of steps:\n",
    "\n",
    "- `samtools view -bS` : SAM → BAM\n",
    "- `samtools sort` : coordinate-sort\n",
    "- `samtools index` : build .bai index\n",
    "- `samtools flagstat` : quick stats (mapped/unmapped etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bam_paths(sam_path: Path):\n",
    "    bam = sam_path.with_suffix(\".bam\")\n",
    "    sortbam = sam_path.with_suffix(\".sorted.bam\")\n",
    "    return bam, sortbam\n",
    "\n",
    "bam_def, sort_def = bam_paths(sam_default)\n",
    "bam_vsl, sort_vsl = bam_paths(sam_vsl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default BAM\n",
    "!samtools view -bS \"{sam_default}\" > \"{bam_def}\"\n",
    "!samtools sort -o \"{sort_def}\" \"{bam_def}\"\n",
    "!samtools index \"{sort_def}\"\n",
    "\n",
    "# Very-sensitive-local BAM\n",
    "!samtools view -bS \"{sam_vsl}\" > \"{bam_vsl}\"\n",
    "!samtools sort -o \"{sort_vsl}\" \"{bam_vsl}\"\n",
    "!samtools index \"{sort_vsl}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c51d6",
   "metadata": {},
   "source": [
    "### SAM field guide\n",
    "\n",
    "| Col | Field | Type   | Brief description                         |\n",
    "|-----|-------|--------|--------------------------------------------|\n",
    "| 1   | QNAME | String | Query template NAME                        |\n",
    "| 2   | FLAG  | Int    | bitwise FLAG                               |\n",
    "| 3   | RNAME | String | Reference sequence NAME                    |\n",
    "| 4   | POS   | Int    | 1-based leftmost mapping POSition          |\n",
    "| 5   | MAPQ  | Int    | MAPping Quality                            |\n",
    "| 6   | CIGAR | String | CIGAR string                               |\n",
    "| 7   | RNEXT | String | Ref. name of the mate/next read            |\n",
    "| 8   | PNEXT | Int    | Position of the mate/next read             |\n",
    "| 9   | TLEN  | Int    | observed Template LENgth                   |\n",
    "| 10  | SEQ   | String | segment SEQuence                           |\n",
    "| 11  | QUAL  | String | ASCII of Phred-scaled base QUALity + 33    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb504802",
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools view -h \"{sort_def}\" | head -n 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1c112",
   "metadata": {},
   "source": [
    "Below, we will examine our CIGAR strings in more detail!\n",
    "\n",
    "Read about CIGAR strings here: https://en.wikipedia.org/wiki/Sequence_alignment#Representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7942a41",
   "metadata": {},
   "source": [
    "### Aside: Running shell commands from Python with `subprocess`\n",
    "\n",
    "Python's `subprocess` module lets you run external command-line programs (like `samtools`) and capture their output. It's the standard replacement for older approaches like `os.system()`.\n",
    "\n",
    "#### The two most common patterns:\n",
    "\n",
    "**1) Capture a command's stdout as a string (simple):**\n",
    "- Use `subprocess.check_output(...)` when you want the command output *and* you want Python to raise an error if the command fails (non-zero exit code).\n",
    "\n",
    "```python\n",
    "out = subprocess.check_output(\n",
    "    \"samtools view my.bam | cut -f 6\",\n",
    "    shell=True,     # lets the pipe `|` work (runs via the shell)\n",
    "    text=True       # returns a Python string instead of bytes\n",
    ")\n",
    "```\n",
    "\n",
    "**2) `subprocess.run` for more control:**\n",
    "\n",
    "```python\n",
    "res = subprocess.run(\n",
    "    [\"samtools\", \"view\", \"my.bam\"],     # no shell; safer and faster\n",
    "    check=True,                         # raise error if rc != 0\n",
    "    text=True,\n",
    "    capture_output=True                 # captures stdout/stderr\n",
    ")\n",
    "stdout = res.stdout\n",
    "```\n",
    "\n",
    "#### More Notes:\n",
    "\n",
    "`shell=True` vs list-of args:\n",
    "\n",
    "- `shell=True` is convenient for pipes `|`, redirection `>`, globbing `*`, etc.\n",
    "- if any part of command comes from user input, perfer list-of-args to avoid shell-injection problems\n",
    "\n",
    "\n",
    "**Error handling and debugging:**\n",
    "- With check_output / run(check=True), failures raise `subprocess.CalledProcessError`.\n",
    "- Capturing stderr can help diagnose missing tools, bad paths, etc.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    res = subprocess.run([\"samtools\", \"--version\"], check=True, text=True, capture_output=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Command failed:\", e)\n",
    "    print(\"stderr:\", e.stderr)\n",
    "```\n",
    "\n",
    "In the code below, we use `check_output(..., shell=True, text=True)` because we're using a shell pipeline (`samtools ... | cut ...`), and we want the resulting text output in Python for parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cigar_summary(bam_path, max_lines=None):\n",
    "    # Grab CIGAR (field 6) from samtools view\n",
    "\n",
    "    # here is the command\n",
    "    cmd = f\"samtools view {bam_path} | cut -f 6\"\n",
    "    if max_lines:\n",
    "        cmd += f\" | head -n {int(max_lines)}\"\n",
    "\n",
    "    # here is the use of subprocess\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "    cigars = [c.strip() for c in out.splitlines() if c.strip()]\n",
    "\n",
    "    trivial_re = re.compile(r\"^\\d+M$\")   # exactly like \"101M\"\n",
    "    is_trivial = [bool(trivial_re.match(c)) for c in cigars]\n",
    "\n",
    "    return {\n",
    "        \"n_total\": len(cigars),\n",
    "        \"n_trivial_M_only\": sum(is_trivial),\n",
    "        \"n_nontrivial\": len(cigars) - sum(is_trivial),\n",
    "    }\n",
    "\n",
    "print(\"DEFAULT:\", cigar_summary(sort_def))\n",
    "print(\"VSL-LOC:\", cigar_summary(sort_vsl))\n",
    "\n",
    "def nontrivial_cigar_table(bam_path, n=20):\n",
    "    # Pull key fields: QNAME FLAG RNAME POS MAPQ CIGAR\n",
    "\n",
    "    # here is the command!\n",
    "    cmd = f\"samtools view {bam_path} | cut -f 1-6\"\n",
    "\n",
    "    # Here's the use of subprocess\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "\n",
    "    rows = []\n",
    "    trivial_re = re.compile(r\"^\\d+M$\")\n",
    "    for line in out.splitlines():\n",
    "        qname, flag, rname, pos, mapq, cigar = line.split(\"\\t\")\n",
    "        if cigar != \"*\" and not trivial_re.match(cigar):\n",
    "            rows.append((qname, int(flag), rname, int(pos), int(mapq), cigar))\n",
    "            if len(rows) >= n:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"QNAME\",\"FLAG\",\"RNAME\",\"POS\",\"MAPQ\",\"CIGAR\"])\n",
    "\n",
    "df_nontriv_vsl = nontrivial_cigar_table(sort_vsl, n=10)\n",
    "df_nontriv_vsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4d48c",
   "metadata": {},
   "source": [
    "## MAPQ confidence\n",
    "\n",
    "MAPQ is about how uniquely / confidently Bowtie2 placed the read.\n",
    "- High MAPQ: “I'm pretty sure this is the right spot”\n",
    "- Low MAPQ: “This read could map equally well to multiple places”\n",
    "\n",
    "We will compare histograms for default vs. a different setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748944d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_mapq(bam_path):\n",
    "    cmd = f\"samtools view {bam_path} | cut -f 5\"\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "    vals = [int(x) for x in out.strip().split() if x.strip()]\n",
    "    return np.array(vals, dtype=int)\n",
    "\n",
    "mapq_def = extract_mapq(sort_def)\n",
    "mapq_vsl = extract_mapq(sort_vsl)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(mapq_def, bins=range(0, 61, 1))\n",
    "plt.title(\"MAPQ histogram — Bowtie2 default\")\n",
    "plt.xlabel(\"MAPQ\")\n",
    "plt.ylabel(\"Read count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(mapq_vsl, bins=range(0, 61, 1))\n",
    "plt.title(\"MAPQ histogram — Bowtie2 --very-sensitive-local\")\n",
    "plt.xlabel(\"MAPQ\")\n",
    "plt.ylabel(\"Read count\")\n",
    "plt.show()\n",
    "\n",
    "print(\"DEFAULT:  n=\", len(mapq_def), \"  fraction MAPQ>=30 =\", (mapq_def>=30).mean())\n",
    "print(\"VSL-LOC:  n=\", len(mapq_vsl), \"  fraction MAPQ>=30 =\", (mapq_vsl>=30).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18fae1",
   "metadata": {},
   "source": [
    "## Compute coverage across locus\n",
    "\n",
    "Coverage answers: \"How many reads overlap each position?\"\n",
    "\n",
    "Sanity check:\n",
    "- coverage should be approximately smooth for a well-behaving locus\n",
    "- for repetitive loci, we can get spikes/weird patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def coverage_df(bam_path: Path):\n",
    "    cmd = f\"samtools depth -aa {bam_path}\"\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "    rows = []\n",
    "    for line in out.strip().splitlines():\n",
    "        rname, pos, depth = line.split(\"\\t\")\n",
    "        rows.append((rname, int(pos), int(depth)))\n",
    "    return pd.DataFrame(rows, columns=[\"rname\",\"pos\",\"depth\"])\n",
    "\n",
    "cov_def = coverage_df(sort_def)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cov_def[\"pos\"], cov_def[\"depth\"])\n",
    "plt.title(\"Coverage across locus — Bowtie2 default\")\n",
    "plt.xlabel(\"Position on reference (1-based)\")\n",
    "plt.ylabel(\"Depth\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a51db",
   "metadata": {},
   "source": [
    "# Checkpoint - answer in MITx\n",
    "\n",
    "What is the average coverage across the loci for the default setting?\n",
    "\n",
    "Round to 1 significant digit.\n",
    "\n",
    "Hint: use the `describe()` function to explore your `pandas` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4c386",
   "metadata": {},
   "source": [
    "## Batch alignment pipeline\n",
    "\n",
    "Run bowtie2 default for all samples.\n",
    "\n",
    "This is an example pipeline!\n",
    "\n",
    "1. Locate the trimmed FASTQ from Trim Galore\n",
    "2. Build a Bowtie2 index for the sample's locus reference\n",
    "3. Align with Bowtie2 default settings\n",
    "4. Convert to sorted BAM + index\n",
    "5. Collect a small summary table (overall alignment rate, etc.)\n",
    "\n",
    "\n",
    "This gives us a consistent baseline to compare samples—especially before we start tweaking alignment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess, re\n",
    "import pandas as pd\n",
    "\n",
    "WORK = Path(\"work\")\n",
    "\n",
    "def sh(cmd: str):\n",
    "    \"\"\"Run a shell command; raise if it fails; return CompletedProcess.\"\"\"\n",
    "    return subprocess.run(cmd, shell=True, check=True, text=True, capture_output=True)\n",
    "\n",
    "def find_trimmed_fastq(case: int, sample: str) -> Path:\n",
    "    trim_dir = WORK / f\"case{case}_{sample}\" / \"trim_galore\"\n",
    "    assert trim_dir.exists(), f\"Missing {trim_dir}. Run trimming first.\"\n",
    "    trimmed = list(trim_dir.glob(\"*_trimmed.fq.gz\"))\n",
    "    assert len(trimmed) == 1, f\"Expected 1 trimmed FASTQ in {trim_dir}, found: {trimmed}\"\n",
    "    return trimmed[0]\n",
    "\n",
    "def ensure_bowtie2_index(ref_fa: Path, idx_prefix: Path):\n",
    "    \"\"\"Build bowtie2 index only if it doesn't exist.\"\"\"\n",
    "    idx_dir = idx_prefix.parent\n",
    "    idx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # bowtie2-build outputs files like ref.1.bt2 (or .bt2l). We'll check for either.\n",
    "    bt2_files = list(idx_dir.glob(idx_prefix.name + \"*.bt2\")) + list(idx_dir.glob(idx_prefix.name + \"*.bt2l\"))\n",
    "    if len(bt2_files) >= 4:\n",
    "        return  # index exists\n",
    "\n",
    "    print(f\"[index] building: {idx_prefix}\")\n",
    "    sh(f'bowtie2-build \"{ref_fa}\" \"{idx_prefix}\"')\n",
    "\n",
    "def parse_bowtie2_log(log_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Pull out a few useful metrics from bowtie2 stderr summary.\n",
    "    Works with standard bowtie2 summary format.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    m = re.search(r\"(\\d+(?:\\.\\d+)?)% overall alignment rate\", log_text)\n",
    "    if m:\n",
    "        out[\"overall_alignment_rate_pct\"] = float(m.group(1))\n",
    "\n",
    "    m = re.search(r\"(\\d+(?:\\.\\d+)?)% aligned 0 times\", log_text)\n",
    "    if m:\n",
    "        out[\"aligned_0x_pct\"] = float(m.group(1))\n",
    "\n",
    "    m = re.search(r\"(\\d+(?:\\.\\d+)?)% aligned exactly 1 time\", log_text)\n",
    "    if m:\n",
    "        out[\"aligned_1x_pct\"] = float(m.group(1))\n",
    "\n",
    "    m = re.search(r\"(\\d+(?:\\.\\d+)?)% aligned >1 times\", log_text)\n",
    "    if m:\n",
    "        out[\"aligned_gt1x_pct\"] = float(m.group(1))\n",
    "\n",
    "    return out\n",
    "\n",
    "def sam_to_sorted_bam(sam_path: Path, out_prefix: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Convert SAM->BAM->sorted BAM and index.\n",
    "    Returns sorted BAM path.\n",
    "    \"\"\"\n",
    "    bam = out_prefix.with_suffix(\".bam\")\n",
    "    sortbam = out_prefix.with_suffix(\".sorted.bam\")\n",
    "\n",
    "    sh(f'samtools view -bS \"{sam_path}\" > \"{bam}\"')\n",
    "    sh(f'samtools sort -o \"{sortbam}\" \"{bam}\"')\n",
    "    sh(f'samtools index \"{sortbam}\"')\n",
    "    return sortbam\n",
    "\n",
    "def run_bowtie2_default(case: int, sample: str) -> dict:\n",
    "    \"\"\"\n",
    "    Runs bowtie2 default pipeline for a given (case, sample).\n",
    "    Saves:\n",
    "      work/case{case}_{sample}/align_default/\n",
    "        - aln.default.sam\n",
    "        - aln.default.sam.log\n",
    "        - aln.default.sorted.bam (+ .bai)\n",
    "    \"\"\"\n",
    "    case_dir = WORK / f\"case{case}_{sample}\"\n",
    "    out_dir = case_dir / \"align_default\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ref_fa = Path(FILES[(case, sample)][\"ref\"])\n",
    "    trimmed_fq = find_trimmed_fastq(case, sample)\n",
    "\n",
    "    idx_dir = case_dir / \"bowtie2_index\"\n",
    "    idx_prefix = idx_dir / \"ref\"\n",
    "    ensure_bowtie2_index(ref_fa, idx_prefix)\n",
    "\n",
    "    sam_path = out_dir / \"aln.default.sam\"\n",
    "    log_path = out_dir / \"aln.default.sam.log\"\n",
    "    bam_prefix = out_dir / \"aln.default\"\n",
    "\n",
    "    print(f\"[align] case{case} {sample}\")\n",
    "    # Capture bowtie2 stderr (summary) into a log file\n",
    "    sh(f'bowtie2 -x \"{idx_prefix}\" -U \"{trimmed_fq}\" -S \"{sam_path}\" 2> \"{log_path}\"')\n",
    "\n",
    "    # Convert to sorted BAM\n",
    "    sortbam = sam_to_sorted_bam(sam_path, bam_prefix)\n",
    "\n",
    "    log_text = log_path.read_text()\n",
    "    metrics = parse_bowtie2_log(log_text)\n",
    "\n",
    "    # Add a couple of paths for convenience\n",
    "    metrics.update({\n",
    "        \"case\": case,\n",
    "        \"sample\": sample,\n",
    "        \"ref_fa\": str(ref_fa),\n",
    "        \"trimmed_fq\": str(trimmed_fq),\n",
    "        \"sorted_bam\": str(sortbam),\n",
    "        \"bowtie2_log\": str(log_path),\n",
    "    })\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for all configured case/sample combinations\n",
    "results = []\n",
    "for (case, sample) in sorted(FILES.keys()):\n",
    "    results.append(run_bowtie2_default(case, sample))\n",
    "\n",
    "df = pd.DataFrame(results).sort_values([\"case\", \"sample\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ba6a7",
   "metadata": {},
   "source": [
    "## Complete questions in MITx on this section!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc2f6c",
   "metadata": {},
   "source": [
    "# Pileups & Variant Calling\n",
    "\n",
    "\n",
    "In this section we connect three ideas:\n",
    "\n",
    "1. **Pileup = evidence table**  \n",
    "   A pileup is a per-position summary of what the reads “say” at each reference base:\n",
    "   - the reference base at that position\n",
    "   - how many reads cover it (depth)\n",
    "   - what bases those reads contain (and their qualities)\n",
    "\n",
    "2. **Variant calling = rules/models applied to pileup evidence**  \n",
    "   Tools like **bcftools** look across the pileup and decide whether the data supports a real difference from the reference sequence.\n",
    "\n",
    "3. **Visualization = sanity check**  \n",
    "   Even if a caller outputs a VCF, it's valuable to *see* the evidence in a pileup view.\n",
    "\n",
    "\n",
    "\n",
    "## Create reference FASTA index\n",
    "\n",
    "We aligned to the locus FASTA, so our BAM coordinates are local to that locus contig.\n",
    "\n",
    "We will use `samtools` to get that fasta index `*.fai` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fdbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools faidx data/loci/locus1_hg38_chr11_5215464_5237071_HBB.fa\n",
    "!cut -f1,2 data/loci/locus1_hg38_chr11_5215464_5237071_HBB.fa.fai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027ca47",
   "metadata": {},
   "source": [
    "Now confirm the BAM is aligned against the same contig name.\n",
    "\n",
    "The .fai file lists the contig name and length. This contig name must match what is stored in the BAM header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "def sh(cmd: str) -> str:\n",
    "    return subprocess.check_output(cmd, shell=True, text=True)\n",
    "\n",
    "REF = Path(\"data/loci/locus1_hg38_chr11_5215464_5237071_HBB.fa\")\n",
    "BAM1 = Path(\"work/case1_ind1_noHbS/align_default/aln.sorted.bam\")\n",
    "BAM2 = Path(\"work/case1_ind2_HbS_carrier/align_default/aln.sorted.bam\")\n",
    "\n",
    "assert REF.exists(), REF\n",
    "assert BAM1.exists(), BAM1\n",
    "assert BAM2.exists(), BAM2\n",
    "\n",
    "# contig name from FASTA index\n",
    "ref_contig = sh(f\"cut -f1 {REF}.fai | head -n 1\").strip()\n",
    "print(\"Contig in FASTA:\", ref_contig)\n",
    "\n",
    "case = 1\n",
    "for BAM in [BAM1, BAM2]:\n",
    "    contig = sh(\n",
    "        f\"samtools view -H {BAM} | \"\n",
    "        \"awk -F'\\\\t' '$1==\\\"@SQ\\\"{for(i=1;i<=NF;i++) if($i~\\\"^SN:\\\"){sub(\\\"SN:\\\",\\\"\\\",$i); print $i; exit}}'\"\n",
    "    ).strip()\n",
    "    print(f\"Contig in BAM for case {case}:\", contig)\n",
    "    assert contig, \"Could not find contig name in BAM header\"\n",
    "    assert contig == ref_contig, f\"Contig mismatch: BAM has {contig}, FASTA has {ref_contig}\"\n",
    "    case += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d47159a",
   "metadata": {},
   "source": [
    "## Discovery step: Call variants with bcftools\n",
    "\n",
    "\n",
    "In patient 2, can we find a variant that causes sickle cell?\n",
    "\n",
    "\n",
    "We use the standard small-variant workflow:\n",
    "- `bcftools mpileup` computes genotype likelihoods from the alignments (the pileup evidence)\n",
    "- `bcftools call -mv` calls SNPs/indels as variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call variants (VCF text to stdout), then show non-header lines\n",
    "!bcftools mpileup -f {REF} -Ou {BAM1} \\\n",
    "  | bcftools call -mv -Ov \\\n",
    "  | grep -v '^##' || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822add92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bcftools mpileup -f {REF} -Ou {BAM2} \\\n",
    "  | bcftools call -mv -Ov \\\n",
    "  | grep -v '^##' || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cb43d",
   "metadata": {},
   "source": [
    "It looks like we found a variant in our sickle cell trait carrier. Let's further examine to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "called_pos = sh(\n",
    "    f\"bcftools mpileup -f {REF} -Ou {BAM2} \"\n",
    "    f\"| bcftools call -mv -Ov \"\n",
    "    f\"| bcftools view -H \"\n",
    "    f\"| cut -f2 | head -n 1\"\n",
    ").strip()\n",
    "\n",
    "print(\"Called position:\", called_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470af700",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = int(called_pos)\n",
    "for BAM in [BAM1, BAM2]:\n",
    "  print(f\"{BAM}:\")\n",
    "  !samtools mpileup -f {REF} -r {contig}:{pos}-{pos} -q 0 -Q 0 {BAM}\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b284b78",
   "metadata": {},
   "source": [
    "### ASCII pileup screenshoot\n",
    "\n",
    "To see the evidence around the called site, we use\n",
    "\n",
    "```bash\n",
    "samtools tview -d T -p 'CONTIG:POS' sample.bam REF.fa > pileup.txt\n",
    "```\n",
    "\n",
    "This produces a text display where:\n",
    "- The top sequence line is the reference\n",
    "- Each subsequent line is a read “row”\n",
    "- `.` means the read base matches the reference at that column\n",
    "- Letters like A, C, G, T indicate mismatches (candidate alternate alleles)\n",
    "- You may also see IUPAC ambiguity codes:\n",
    "     - Read about IUPAC base symbols here: https://en.wikipedia.org/wiki/Nucleic_acid_notation\n",
    "\n",
    "This view is a quick way to ask:\n",
    "- Do many reads support the same alternate base at the same position?\n",
    "- Is the evidence confined to one strand, near read ends, or low quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c737dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for case_num, BAM in enumerate([BAM1, BAM2]):\n",
    "  out_txt = Path(f\"pileup_ind{case_num+1}_called_site.txt\")\n",
    "  sh(f\"samtools tview -d T -p '{contig}:{pos-40}' {BAM} {REF} > {out_txt}\")\n",
    "\n",
    "  print(\"Wrote:\", out_txt)\n",
    "  print(sh(f\"sed -n '1,80p' {out_txt}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6b6d4",
   "metadata": {},
   "source": [
    "## Variant interpretation\n",
    "\n",
    "HBB HbS = rs334: a single-base change in HBB that changes codon 6 from Glu to Val (E6V), which is listed as the classic sickle-cell variant.\n",
    "\n",
    "\n",
    "This was actually discovered before genetic sequencing in 1956.\n",
    "\n",
    "https://www.nature.com/articles/178792a0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78926f03",
   "metadata": {},
   "source": [
    "## Answer the questions in MITx on the Pileup & Variant Calling section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb41ed",
   "metadata": {},
   "source": [
    "# Probability in DNA-seq analysis\n",
    "\n",
    "How does error rate and sequencing depth affect genome assembly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba2ac5",
   "metadata": {},
   "source": [
    "We are assembling a bacteria. Unlike humans, it is haploid (only 1 chromosome copy per gene).\n",
    "\n",
    "Let's use this haploid genome to explore how sequencing depth & error rate affect the outcome.\n",
    "\n",
    "At a single haploid genome position, the true base is fixed (no heterozygotes).\n",
    "\n",
    "Each read is wrong with probability $p$, independently across reads. Let $C$ be the sequencing depth (number of reads covering the site), and let $K$ be the number of wrong reads.\n",
    "\n",
    "Because each read is a Bernoulli trial (wrong vs. correct), we have a binomial distribution: https://en.wikipedia.org/wiki/Binomial_distribution\n",
    "\n",
    "$K \\sim \\text{Binomial}(C,p)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fcf0eb",
   "metadata": {},
   "source": [
    "**Probability the consensus is wrong**\n",
    "\n",
    "Assume conensus is determined by majority vote, the consensus is wrong only if more than half the reads are wrong:\n",
    "\n",
    "$P(\\text{wrong consensus at depth } C) = P(K > \\frac{C}{2}) = \\sum_{k=[C/2]+1}^{C}{C\\choose k} p^k (1-p)^{C-k}$\n",
    "\n",
    "\n",
    "It's easier to just use the cumulative distribution function ($F(.)$):\n",
    "\n",
    "$P(\\text{wrong})=1-F([C/2]; C,p)$\n",
    "\n",
    "\n",
    "In the below cell below, use `scipy` to return the correct probability:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaec1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "def p_wrong_consensus(C, p):\n",
    "    \"\"\"\n",
    "    Majority vote is wrong iff K > C/2, where K ~ Binomial(C, p).\n",
    "    So P(wrong) = P(K >= floor(C/2)+1) = 1 - BinomCDF(floor(C/2)).\n",
    "    \"\"\"\n",
    "    return 1 - stats.binom.cdf(C//2, C, p)\n",
    "\n",
    "def find_min_depth(p, target=1e-4, max_C=500):\n",
    "    for C in range(1, max_C + 1):\n",
    "        if p_wrong_consensus(C, p) <= target:\n",
    "            return C\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3361a4d8",
   "metadata": {},
   "source": [
    "## TODO Plot for Illumina\n",
    "\n",
    "Use $p=0.001$\n",
    "\n",
    "Plot $P(\\text{wrong})$ vs. $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot for Illumina\n",
    "\n",
    "p_illumina = 0.001\n",
    "target = 1e-4\n",
    "max_C = 50\n",
    "Cs = np.arange(1, max_C + 1)\n",
    "\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a6c64",
   "metadata": {},
   "source": [
    "Do the same for Nanopore given\n",
    "\n",
    "$p=0.03$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot for Nanopore\n",
    "\n",
    "p_nanopore = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fbecf",
   "metadata": {},
   "source": [
    "About how much more depth is needed to reach target probability that the consensus is wrong?\n",
    "\n",
    "\n",
    "Nanopore has a 30x fold higher error rate than Illumina. Do we need 30-fold higher coverage? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e4c5c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
